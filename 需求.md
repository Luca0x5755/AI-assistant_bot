根據以下需求 MVP 理解@01_development_workflow_cookbook.md 
產出
@02_project_brief_and_prd.md @03_behavior_driven_development_guide.md@05_architecture_and_design_document.md

嗨維陽～
我們團隊對於您的經歷非常有興趣
很希望可以有機會進一步
感謝您願意嘗試作業
這是我們的作業內容

以下所有需求都要用地端模型
- 基本的 demo 用聊天室
- 聊天室要在用戶講話後調用 LLM 相關的功能完成自動對話
- 讓用戶可以用 STT 輸入
- STT的語音要存起來用VAD/ASR等技術做語音分析，分析出說話者的自信度、平穩度、親和力等

- 對於 AI 的回應要套用 cloneTTS 做回應 ( 畫面上要有地方可以上傳要複製的聲音 )

- 使用 Clone TTS 可以提供兩種替代方案
速度快、品質低：50字內文字轉成音檔的時間在3秒內
速度慢、品質高：台灣口音自然、還原上傳音檔的音色
- 轉音連音自然、若上傳音檔有情緒起伏克隆效果也能有情緒起伏

- 根據實測你的模型效果後推估
如果要用以下機器的話你會怎麼去分配資源
採購多少量、做什麼樣的升級更換等邏輯
如何調配上述所有模型所佔用的資源來選定合適的機器規格


估算機器規格

內裝 RTX 4090 VRAM 24GB
https://www.aaeon.com/en/product/detail/edge-ai-server-maxer-2100

規格如下:
CPU:i7-13700
Memory:64GB
Storage:1T SSD
GPU:NVIDIA RTX-4090 -> VRAM 24GB
作業系統:Linux

**一句話結論｜**先快後美——MVP 採 **Whisper(CT2) 串流 ASR + vLLM 7–8B 級 LLM +「快/美」雙檔 Clone-TTS**，單卡 4090 以佇列與切句壓延遲≤3s；**拒絕 70B 動態換艙**於 MVP，等雙機或 48GB GPU 再上高質同步。

---

# 需求分析(SA) × 技術設計(SD) 收斂方案

## 0. 決策總覽（Decision Summary）

| 面向         | 選定方案                                                                 | 不採用於 MVP 的方案                       | 理由                                                            |
| ---------- | -------------------------------------------------------------------- | ---------------------------------- | ------------------------------------------------------------- |
| **LLM**    | **vLLM + Qwen2.5-7B / Llama-3.1-8B（INT4/FP8）**                       | llama.cpp + Llama-3-70B（VRAM 動態交換） | 70B 需頻繁卸/載 VRAM，HQ-TTS 與 LLM 互搶顯存，**E2E 延遲>20s**；MVP 體驗與風險不符。 |
| **STT**    | **faster-whisper large-v3(-turbo) + Silero/WebRTC VAD（CT2 流式、INT8）** | GPU 常駐多 ASR                        | CT2 在 CPU/GPU 均快、省顯存，與 LLM 互不干擾；支援流式與內建 VAD。                  |
| **TTS（快）** | **F5-TTS / MeloTTS（切句、邊生邊播，≤50字≲3s）**                                | 以高質模型硬撐即時                          | 非自回歸/輕量聲碼器，啟動快；用於**先回聲**。                                     |
| **TTS（高）** | **CosyVoice 3 / XTTS v2 / Fish-Speech（零樣本克隆、情緒/口音）**                 | 與 LLM 同卡常駐                         | 單卡 24GB 下改為**按需載入**；HQ 走「美聲檔」，快檔維持對話流暢。                       |
| **語音分析**   | **openSMILE / Parselmouth + Librosa（CPU）**                           | 端上深網多模型                            | 先用可解釋韻律指標建 proxy 分數，後續再以輕量回歸頭校準。                              |

---

## 1. 目標與 SLO（MVP）

* **端到端（50字）**：ASR 串流 < 0.6s → LLM **TTFT < 0.6s** → TTS(快) < 1.8s → **E2E ≤ 3.0s**
* **並發**：單 4090 **3–5 會話**穩定；P95 TTFT < 800ms
* **硬體**：i7-13700 / 64GB RAM / 1TB SSD / **RTX 4090(24GB)**
* **地端**：**無雲 API**；Linux；模型與音檔本地留存

---

## 2. 最小可行架構（MVP HLD）

```
Web(React+WS/WebRTC)
   └─ Mic/Upload(聲紋) → WS → FastAPI Orchestrator
        ├─ ASR: faster-whisper(CT2, streaming, VAD)
        ├─ LLM: vLLM(7–8B, INT4/FP8, paged-attn, cont. batching)
        ├─ TTS-fast: F5-TTS/MeloTTS（切句、邊生邊播）
        ├─ TTS-hq: CosyVoice3 / XTTS v2 / Fish-Speech（按需載入）
        ├─ Prosody: openSMILE/Parselmouth/Librosa（CPU, async）
        ├─ Store: MinIO(音檔) + Postgres(特徵/分數/日誌)
        └─ Metrics: Prometheus + Grafana（TTFT/RTF/KV/佇列）
```

**VRAM 規劃（常駐）**：LLM 8–12GB（INT4/FP8）+ 余量給 KV；ASR 主要跑 CPU/少量 GPU；TTS-fast **按需上卡**（短駐留）；TTS-HQ **不常駐**（使用時載入、生成後卸載）。

---

## 3. 關鍵設計要點

### 3.1 流程（切句與佇列保即時）

1. **VAD 切段**（300–600ms chunk），ASR **partial** 字幕即時回傳。
2. **LLM** 收到 partial/sentence 即刻生成，**chunked prefill** 降 TTFT。
3. **TTS-fast** 對 10–15 字 **分句合成邊生邊播**；同時排隊生成 **TTS-HQ** 完整句覆蓋。
4. Prosody 分析 **異步** 跑，寫入分數與特徵。

### 3.2 Prosody 指標（先可解釋、後校準）

* **語速** (r = \frac{\text{syllables}}{\text{second}})，**停頓比** (p = \frac{T_{\text{sil}}}{T_{\text{total}}})
* **穩定度**：( \text{jitter} = \frac{1}{N-1}\sum\frac{|T_{i+1}-T_i|}{\overline{T}} ),
  ( \text{shimmer} = \frac{1}{N-1}\sum\frac{|A_{i+1}-A_i|}{\overline{A}} )
* **語調/親和**：(f_0) 範圍、輪廓變化率、HNR、能量動態
* **分數**：( s = \sigma!\left(w^\top x\right) )，以 100–300 條域內標註做弱監督回歸頭校準（**相對分數**優先）。

### 3.3 API（型別嚴謹，便於前後端對接）

```python
# Pydantic 模型摘要
class STTChunk(BaseModel):
    session_id: str
    pcm16: bytes
    sr: int = 16000
    ts_ms: int
    vad: bool = True

class LLMReq(BaseModel):
    session_id: str
    messages: list[dict]
    max_tokens: int = 256
    stream: bool = True

class TTSReq(BaseModel):
    session_id: str
    text: str
    mode: Literal["fast","hq"] = "fast"
    voice_id: str | None = None
    stream: bool = True

class ProsodyScore(BaseModel):
    session_id: str
    confidence: float
    stability: float
    affability: float
    features: dict  # f0/jitter/shimmer/r/p 等
```

### 3.4 參數與啟動（參考）

* **vLLM**：`--gpu-memory-utilization 0.85 --max-model-len 4096 --kv-cache-dtype fp8 --quantization awq|fp8`
* **CT2**：`compute_type=int8`, `vad_filter=True`, `min_silence=300–500ms`, `beam_size=1–3`
* **TTS-fast**：預熱 + 文本切句（標點/停頓），每句 10–15 字；流式回放。
* **TTS-HQ**：按需載入（任務佇列 + 熱度驅逐策略 LRU）。

---

## 4. 部署與擴展

### 4.1 Docker Compose（MVP）

```yaml
services:
  orchestrator: { image: app, ports: ["8000:8000"], depends_on: [vllm, asr, store] }
  vllm:
    image: vllm
    command: ["--model","Qwen2.5-7B-Instruct","--gpu-memory-utilization","0.85",
              "--kv-cache-dtype","fp8","--quantization","awq"]
    deploy: { resources: { reservations: { devices: [{driver: nvidia, count: 1, capabilities: [gpu]}]}}}
  asr: { image: ct2-asr, environment: ["THREADS=8"] }
  tts_fast: { image: f5-tts, deploy: { resources: { reservations: { devices: [{driver: nvidia, count: 1, capabilities: [gpu]}]}}}}
  tts_hq: { image: cosyvoice3, deploy: { resources: { reservations: { devices: [{driver: nvidia, count: 1, capabilities: [gpu]}]}}}, deploy_on_demand: true }
  store: { image: minio }
  db: { image: postgres }
  mon: { image: prometheus-grafana }
```

### 4.2 升級路線（3 檔）

1. **檔位 A：單機整合（現在）**——3–5 會話；HQ 檔按需載入。
2. **檔位 B：雙機分工（LLM / Speech）**——10–15 會話；HQ 常駐，延遲更穩。
3. **檔位 C：48GB GPU 或多機**——**LLM+HQ-TTS 同卡常駐**；去除載入開銷；可引入 Triton、多實例、水平擴。

---

## 5. 驗收與量測（必備儀表）

* **LLM**：TTFT、tokens/s、KV 告警（OOM/fragmentation）
* **ASR**：RTF、WER/CER（抽樣標註）
* **TTS**：快檔產出延遲（P50/P95）、HQ 載入耗時
* **E2E**：對話輪延遲分佈、抖動；會話併發下的 P95
* **分析**：jitter/shimmer/f0_range/語速/停頓比 的分位數與重測信度

**MVP 驗收門檻**

* P95 **E2E ≤ 3.5s（50字）**；P95 TTFT ≤ 800ms；快檔 P50 ≤ 1.5s
* 連續 2 小時壓測（5 會話）無 OOM；語音與特徵100%落盤可追溯

---

## 6. 風險 × 緩解

| 風險          | 徵兆           | 緩解                                                                   |
| ----------- | ------------ | -------------------------------------------------------------------- |
| 顯存吃緊 / OOM  | vLLM 告警、延遲飆升 | **INT4/FP8**，縮 `max_model_len`、限流 `max_num_seqs`、KV FP8；TTS-HQ 改批量生成 |
| 快檔音質差       | 用戶主觀抱怨       | **先快後美**：快檔保節奏，HQ 覆蓋；增 prosody tag、文本正規化                             |
| 台式口音 ASR 誤字 | CER 偏高       | large-v3-turbo、域語彙表、斷詞優化；必要時蒐集小量域內語音做適配                              |
| 指標可用性       | 分數不穩         | 以**相對分數**（同人前後）與重測信度為準，逐步蒐集標註微調回歸頭                                   |

---

## 7. 執行計畫（四週）

* **W1**：FastAPI × WS 串流 + CT2 流式 + vLLM（8B）+ F5-TTS（切句播）；儀表板
* **W2**：Prosody 異步分析 + 分數面板；聲紋上傳與管理
* **W3**：CosyVoice/XTTS/Fish-Speech HQ 管線（按需載入）+ 佇列與熱度驅逐
* **W4**：壓測與參數掃描（INT4/FP8/KV/seq）；SLO 驗收與 Demo

---

## 8. 若要追求「70B + 高質同步」的正確姿勢

* **不建議**在 24GB 上做 VRAM 動態換艙。
* **正解**：**雙機或 48GB 卡**（RTX 6000 Ada）→ LLM + HQ-TTS 同卡常駐；或 LLM 專卡（vLLM）、Speech 專卡（ASR/TTS）。
* 高階再引入 **Triton 多實例**、**LiveKit/WebRTC** 低延遲通道與**工具調用**。

---

# 總結

以 **「先快後美」** 為原則，用 **7–8B + CT2 + 雙檔 TTS** 在單卡 4090 做到**可用且順**；先以**切句+邊生邊播**鎖住 3 秒體感，再以 HQ 檔覆蓋音質。等到要放大併發與音質同步，再上**雙機分工**或**48GB GPU**。這條路徑**風險最低、體驗最佳、可連續交付**。

---

## 心法（五歲也懂）

把系統想成三位夥伴：**聽懂的人（ASR）**、**會回答的人（LLM）**、**說出來的人（TTS）**。
先讓「會回答的人」**快快回一小句**，叫「說出來的人」**先用普通聲音說**；等準備好了，再用**漂亮聲音**補上整句。

## 口訣（3 點）

1. **先快後美**：快檔保體感，HQ 補音質。
2. **分工併發**：LLM 與語音分離，延遲穩。
3. **度量驅動**：看 **TTFT/RTF/KV/佇列** 調參數、決擴容。
